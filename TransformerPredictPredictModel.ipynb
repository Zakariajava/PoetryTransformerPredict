{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "predict.py\n",
    "\n",
    "Carga un modelo Transformer entrenado con SentencePiece, \n",
    "recibe un contexto de entrada y genera texto adicional \n",
    "en árabe (u otro idioma), guardando el resultado en un \n",
    "archivo de texto UTF-8 con un prefijo \\u200f para facilitar \n",
    "la lectura en RTL.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as spm  \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rope_sin_cos(head_dim, block_size, base=10000):\n",
    "    \"\"\"\n",
    "    Construye tensores de seno y coseno para posiciones [0, block_size-1],\n",
    "    asumiendo que 'head_dim' es la dimensión de cada cabeza (debe ser par).\n",
    "\n",
    "    Args:\n",
    "        head_dim (int): Dimensión de cada cabeza (por ejemplo, 64).\n",
    "        block_size (int): Longitud máxima de la secuencia.\n",
    "        base (int): Base para la escala de frecuencia (usualmente 10000).\n",
    "\n",
    "    Returns:\n",
    "        sin (Tensor): Tensor de senos de forma (1, block_size, head_dim//2).\n",
    "        cos (Tensor): Tensor de cosenos de forma (1, block_size, head_dim//2).\n",
    "    \"\"\"\n",
    "    half_dim = head_dim // 2\n",
    "    # Calcular frecuencias para cada componente de la mitad\n",
    "    freq_seq = torch.arange(0, half_dim, dtype=torch.float32)\n",
    "    freq_seq = base ** (2 * freq_seq / (head_dim * 2))  # Escala según la dimensión\n",
    "\n",
    "    positions = torch.arange(block_size, dtype=torch.float32).unsqueeze(1)  # (block_size, 1)\n",
    "    div_term = positions / freq_seq.unsqueeze(0)  # (block_size, half_dim)\n",
    "    \n",
    "    sin = torch.sin(div_term).unsqueeze(0)  # (1, block_size, half_dim)\n",
    "    cos = torch.cos(div_term).unsqueeze(0)  # (1, block_size, half_dim)\n",
    "    return sin, cos\n",
    "\n",
    "def apply_rope(q, k, rope_sin, rope_cos):\n",
    "    \"\"\"\n",
    "    Aplica Rotary Positional Embeddings a los tensores de consulta y clave\n",
    "    de un único head.\n",
    "\n",
    "    Args:\n",
    "        q (Tensor): Tensor de consulta de forma (B, T, head_dim).\n",
    "        k (Tensor): Tensor de clave de forma (B, T, head_dim).\n",
    "        rope_sin (Tensor): Tensores de seno, de forma (1, block_size, head_dim//2).\n",
    "        rope_cos (Tensor): Tensores de coseno, de forma (1, block_size, head_dim//2).\n",
    "\n",
    "    Returns:\n",
    "        q_final, k_final (Tensors): Tensores rotados, de forma (B, T, head_dim).\n",
    "    \"\"\"\n",
    "    B, T, C = q.shape  # C = head_dim\n",
    "    half_dim = C // 2\n",
    "\n",
    "    # Dividir q y k en dos mitades\n",
    "    q_left, q_right = q[..., :half_dim], q[..., half_dim:]\n",
    "    k_left, k_right = k[..., :half_dim], k[..., half_dim:]\n",
    "    \n",
    "    # Seleccionar las posiciones (hasta T) y expandir para tener (B, T, half_dim)\n",
    "    sin_t = rope_sin[:, :T, :].expand(B, T, half_dim)\n",
    "    cos_t = rope_cos[:, :T, :].expand(B, T, half_dim)\n",
    "    \n",
    "    # Aplicar la rotación: \n",
    "    # q_rot = q_left * cos - q_right * sin\n",
    "    # q_pass = q_left * sin + q_right * cos\n",
    "    q_rot = q_left * cos_t - q_right * sin_t\n",
    "    q_pass = q_left * sin_t + q_right * cos_t\n",
    "    q_final = torch.cat((q_rot, q_pass), dim=-1)\n",
    "    \n",
    "    k_rot = k_left * cos_t - k_right * sin_t\n",
    "    k_pass = k_left * sin_t + k_right * cos_t\n",
    "    k_final = torch.cat((k_rot, k_pass), dim=-1)\n",
    "    \n",
    "    return q_final, k_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# ENCABEZADO DEL MODELO\n",
    "# -------------------------------------------------------------------------------\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Cabeza de self-attention con máscara causal y aplicación de RoPE.\n",
    "    \n",
    "    Esta implementación es para un único head; se espera que la dimensión\n",
    "    de entrada 'n_embd' en este módulo corresponda a 'head_dim' (por ejemplo, 64).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, head_size, block_size, dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_embd (int): Dimensión de entrada (debe ser igual a head_size para este head).\n",
    "            head_size (int): Tamaño de cada cabeza (por ejemplo, 64).\n",
    "            block_size (int): Longitud máxima de la secuencia.\n",
    "            dropout (float): Tasa de dropout.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        # Máscara causal: para que cada token no vea tokens futuros.\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, rope_sin, rope_cos):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Entrada de forma (B, T, head_dim), donde head_dim = head_size.\n",
    "            rope_sin, rope_cos (Tensors): Tensores de RoPE con forma (1, block_size, head_dim//2).\n",
    "        \n",
    "        Returns:\n",
    "            out (Tensor): Salida de la cabeza, de forma (B, T, head_size).\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # Aplicar RoPE a q y k\n",
    "        q_rot, k_rot = apply_rope(q, k, rope_sin, rope_cos)\n",
    "        \n",
    "        # Calcular atención escalada\n",
    "        scale = C ** -0.5\n",
    "        wei = q_rot @ k_rot.transpose(-2, -1) * scale  # (B, T, T)\n",
    "        \n",
    "        # Aplicar máscara causal\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Combinar la atención con los valores\n",
    "        out = wei @ v  # (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "# -------------------------------------------------\n",
    "# BLOQUES DEL TRANSFORMER (con FeedForward usando GELU)\n",
    "# -------------------------------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Ejemplo de MultiHeadAttention (sin details de RoPE, \n",
    "    pero podrías integrarlo aquí).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.wq = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.wk = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.wv = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Máscara causal\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Proyecciones\n",
    "        q_full = self.wq(x)  # (B, T, n_embd)\n",
    "        k_full = self.wk(x)\n",
    "        v_full = self.wv(x)\n",
    "\n",
    "        # Dividir heads\n",
    "        q = q_full.view(B, T, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        k = k_full.view(B, T, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        v = v_full.view(B, T, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "\n",
    "        # Atención\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_dim**-0.5)\n",
    "        mask = self.tril[:T, :T]\n",
    "        wei = wei.masked_fill(mask==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        out = wei @ v  # (B, n_heads, T, head_dim)\n",
    "        # Reordenar\n",
    "        out = out.permute(0,2,1,3).contiguous()\n",
    "        out = out.view(B, T, self.n_heads*self.head_dim)\n",
    "        # Proyección final\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP tras la atención, usando GELU en lugar de ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.GELU(),           # Cambiamos ReLU -> GELU\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloque Transformer con:\n",
    "    - MultiHeadAttention\n",
    "    - FeedForward (con GELU)\n",
    "    - Residual connections\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.sa = MultiHeadAttention(n_embd, n_head, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Atención\n",
    "        x_ln = self.ln1(x)\n",
    "        attn_out = self.sa(x_ln)\n",
    "        x = x + attn_out\n",
    "        # FF\n",
    "        x_ln2 = self.ln2(x)\n",
    "        ff_out = self.ffwd(x_ln2)\n",
    "        x = x + ff_out\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de lenguaje con:\n",
    "    - Embeddings\n",
    "    - Bloques (Multi-Head + FeedForward con GELU)\n",
    "    - Funciones de generación con top-k / top-p / temperatura\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, block_size, n_embd, n_head, n_layer, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_embd = n_embd\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # Pila de bloques\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(n_embd, n_head, block_size, dropout) \n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        if T > self.block_size:\n",
    "            raise ValueError(\"Secuencia excede block_size\")\n",
    "\n",
    "        x = self.token_embedding_table(idx)  # (B, T, n_embd)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(\n",
    "        self, \n",
    "        idx, \n",
    "        max_new_tokens: int, \n",
    "        temperature: float=1.0, \n",
    "        top_k: int=None, \n",
    "        top_p: float=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Genera tokens con top-k / top-p sampling y temperatura.\n",
    "        idx: (B, T) secuencia de tokens\n",
    "        max_new_tokens: cuántos tokens generar\n",
    "        temperature: escalado de logits\n",
    "        top_k: mantiene sólo los k más probables\n",
    "        top_p: nucleus sampling, mantiene sólo tokens cumul>= top_p\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Cortar contexto\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            # Tomar logits del último token\n",
    "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "            # Ajustar por temperatura\n",
    "            if temperature != 1.0:\n",
    "                logits = logits / temperature\n",
    "\n",
    "            # Convertir a probabilidades\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, vocab_size)\n",
    "\n",
    "            # top-k\n",
    "            if (top_k is not None) and (top_k > 0):\n",
    "                # Coger los k mejores\n",
    "                values, indices = torch.topk(probs, top_k, dim=-1)\n",
    "                probs_topk = torch.zeros_like(probs).scatter_(-1, indices, values)\n",
    "                probs_topk = probs_topk / probs_topk.sum(dim=-1, keepdim=True)\n",
    "                probs = probs_topk\n",
    "\n",
    "            # top-p (nucleus sampling)\n",
    "            if (top_p is not None) and (0.0 < top_p < 1.0):\n",
    "                sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "                cumulative = torch.cumsum(sorted_probs, dim=-1)\n",
    "                # máscara booleana: qué tokens entran en el top_p\n",
    "                cutoff = (cumulative > top_p).float()\n",
    "                # shift para que el primero que supere top_p se incluya\n",
    "                cutoff = torch.roll(cutoff, 1, dims=-1)\n",
    "                cutoff[..., 0] = 0\n",
    "                # poner 0 en los tokens que exceden top_p\n",
    "                sorted_probs[cutoff.bool()] = 0.0\n",
    "                # renormalizar\n",
    "                sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "                # re-esparcir\n",
    "                probs_top_p = torch.zeros_like(probs).scatter_(-1, sorted_indices, sorted_probs)\n",
    "                probs = probs_top_p\n",
    "\n",
    "            # muestrear\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # concatenar\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding_table): Embedding(9500, 384)\n",
       "  (blocks): ModuleList(\n",
       "    (0-3): 4 x Block(\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (sa): MultiHeadAttention(\n",
       "        (wq): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (wk): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (wv): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=9500, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# CARGAR CHECKPOINT (con RoPE)\n",
    "# -------------------------------------------------------------------------------\n",
    "# Carga del checkpoint\n",
    "checkpoint_path = 'bigram_model_checkpoint.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "# Extraer los hiperparámetros guardados (o usar los que ya tengas definidos)\n",
    "vocab_size = checkpoint['vocab_size']\n",
    "block_size = checkpoint['block_size']\n",
    "n_embd = checkpoint['n_embd']\n",
    "n_head = checkpoint['n_head']\n",
    "n_layer = checkpoint['n_layer']\n",
    "\n",
    "# Instanciar el modelo con los mismos parámetros\n",
    "model = BigramLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=0.2  # o el valor que usaste en train\n",
    ")\n",
    "\n",
    "# Cargar los pesos\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Mover a GPU si hay\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# CARGAR el modelo de SentencePiece usado para tokenizar\n",
    "# -------------------------------------------------------------------------------\n",
    "# Si guardaste la ruta del modelo en el checkpoint, por ejemplo en 'sp_model_file':\n",
    "# sp_model_file = checkpoint['sp_model_file']\n",
    "# Si no, asume que se llama \"arabic_spm.model\".\n",
    "sp_model_file = \"arabic_spm.model\"\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(sp_model_file)\n",
    "\n",
    "# Definimos funciones de encode/decode (Subword)\n",
    "def encode_text(text_str: str):\n",
    "    \"\"\"Convierte un string en subwords IDs usando SentencePiece.\"\"\"\n",
    "    return sp.encode(text_str, out_type=int)\n",
    "\n",
    "def decode_tokens(token_list: list):\n",
    "    \"\"\"Convierte una lista de subword IDs a texto.\"\"\"\n",
    "    return sp.decode(token_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado:\n",
      " ‏بسم الله الرحمن الرحيم يا عثمان يا صانع الذكاء الاصطناعيۭا ينصرون تس أو بفٰحش معكم ٱلذىٓ نزل من ٱلحق ثم ٱلطيرهم بهۦ ۗ وكان ٱلله ٱلقايٰت ٱلله ترجعون لهم أهيمۭ وي بهۦٰه ۚ إنهۥةۢ للذين ٱلرحمٰن فسوفطط متٰعۭا بهۦ ٱليقين ٱل ٱلرشد فى كانوا۟ ٱلقوم يقٰتلوكم يستلقونيهم سيضۭ من قلوبكم ۚ ٱلق ٱلعظيم ۖ فٱذكرون بٱلحق ۚ وما كان ٱدخلوا۟ إذآ ءامنوا۟ ٱشرۢلت من تلك ٱرتضىٰ\n",
      "Resultado guardado en 'resultado_prediccion.txt'.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# RECIBIR CONTEXTO Y GENERAR TEXTO\n",
    "# -------------------------------------------------------------------------------\n",
    "# Ejemplo de contexto\n",
    "# context_input = \"بسم الله الرحمن الرحيم ياأيمن\"\n",
    "context_input = \"بسم الله الرحمن الرحيم يا عثمان يا صانع الذكاء الاصطناعي \"\n",
    "context_encoded = encode_text(context_input)\n",
    "context_tensor = torch.tensor([context_encoded], dtype=torch.long, device=device)\n",
    "\n",
    "max_new_tokens = 70\n",
    "generated_tensor = model.generate(context_tensor, max_new_tokens=max_new_tokens)\n",
    "\n",
    "generated_ids = generated_tensor[0].tolist()\n",
    "generated_text = decode_tokens(generated_ids)\n",
    "\n",
    "arabic_text = \"\\u200f\" + generated_text\n",
    "print(\"Texto generado:\\n\", arabic_text)\n",
    "\n",
    "with open('resultado_prediccion.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(arabic_text)\n",
    "\n",
    "print(\"Resultado guardado en 'resultado_prediccion.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Predicttext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
