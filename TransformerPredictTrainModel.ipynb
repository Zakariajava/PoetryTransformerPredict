{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TransformerPredictTrainModel.ipynb\n",
    "\n",
    "Entrena el modelo BigramLanguageModel utilizando un dataset\n",
    "(leído desde un CSV) y aplica una tokenización subword (BPE)\n",
    "mediante SentencePiece. Al finalizar, guarda los pesos del modelo \n",
    "y varios parámetros necesarios para la inferencia posterior.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# -------------------------------------------------------------------------------\n",
    "# HIPERPARÁMETROS\n",
    "# -------------------------------------------------------------------------------\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "max_iters = 4000\n",
    "eval_interval = 200\n",
    "lr = 3e-4\n",
    "weight_decay = 1e-2\n",
    "grad_accum_steps = 2  # Gradient Accumulation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# NORMALIZACIÓN DE TEXTO (ÁRABE)\n",
    "# -------------------------------------------------------------------------------\n",
    "def normalize_arabic(text):\n",
    "    \"\"\"\n",
    "    Elimina diacríticos y unifica ciertos caracteres.\n",
    "    \"\"\"\n",
    "    # Quita diacríticos.\n",
    "    text = re.sub(r'[\\u0617-\\u061A\\u064B-\\u0652]', '', text)\n",
    "    # (Opcional) Unificar alif: \n",
    "    # text = re.sub(r'[إأآا]', 'ا', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del texto (caracteres) después de normalizar: 39570083\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# LECTURA DE CSV Y GENERACIÓN DE ARCHIVO DE TEXTO\n",
    "# -------------------------------------------------------------------------------\n",
    "df_poetry = pd.read_csv('Arabic_Poetry_Dataset.csv')\n",
    "\n",
    "# Si solo te interesa la columna 'poem_text':\n",
    "raw_text = \"\\n\".join(df_poetry['poem_text'].astype(str).tolist())\n",
    "\n",
    "# Luego normalizas como quieras\n",
    "text_normalized = normalize_arabic(raw_text)\n",
    "\n",
    "print(\"Tamaño del texto (caracteres) después de normalizar:\", len(text_normalized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el corpus en un archivo .txt\n",
    "with open(\"corpus_ar.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# ENTRENAR SENTENCEPIECE (BPE)\n",
    "# -------------------------------------------------------------------------------\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input=corpus_ar.txt '\n",
    "    '--model_prefix=arabic_spm '\n",
    "    '--vocab_size=16000 '     # se proba para obtener mejores resultado (ej. 4k, 8k, 16k)\n",
    "    '--model_type=bpe '      # Byte Pair Encoding\n",
    "    '--character_coverage=1.0 '\n",
    "    '--user_defined_symbols=<pad>,<s>,</s> '\n",
    "    '--unk_piece=<unk>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario (subwords) = 16000\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# CARGAR EL MODELO DE TOKENIZACIÓN\n",
    "# -------------------------------------------------------------------------------\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"arabic_spm.model\")\n",
    "\n",
    "# funciones de encode/decode\n",
    "def encode(s: str):\n",
    "    \"\"\"Convierte un string en una lista de IDs de subword.\"\"\"\n",
    "    return sp.encode(s, out_type=int)\n",
    "\n",
    "def decode(ids: list):\n",
    "    \"\"\"Convierte una lista de IDs en el texto (string).\"\"\"\n",
    "    return sp.decode(ids)\n",
    "\n",
    "# El tamaño de vocab = número de subwords en el modelo\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(\"Tamaño del vocabulario (subwords) =\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud total de tokens (subwords) = 10744625\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# CREAR EL TENSOR DE DATOS (DATA) A PARTIR DEL TEXTO\n",
    "# -------------------------------------------------------------------------------\n",
    "# Convierte el texto normalizado a IDs de subwords\n",
    "encoded_ids = encode(text_normalized)\n",
    "# Monta el tensor PyTorch\n",
    "data = torch.tensor(encoded_ids, dtype=torch.long)\n",
    "print(\"Longitud total de tokens (subwords) =\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# Dividir en train y val (95/10)\n",
    "# -------------------------------------------------------------------------------\n",
    "n = int(0.99 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset de entrenamiento: 10637178\n",
      "Tamaño del dataset de validación: 107447\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamaño del dataset de entrenamiento:\", len(train_data))\n",
    "print(\"Tamaño del dataset de validación:\", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# FUNCIONES PARA OBTENER BATCHES Y ESTIMAR PÉRDIDA\n",
    "# -------------------------------------------------------------------------------\n",
    "def get_batch(split: str):\n",
    "    \"\"\"\n",
    "    Devuelve un batch (x, y) para el conjunto train o val.\n",
    "    \n",
    "    Args:\n",
    "        split (str): 'train' o 'val', para indicar de qué conjunto sacar el batch.\n",
    "        \n",
    "    Returns:\n",
    "        x (tensor): Secuencias de entrada (batch_size, block_size).\n",
    "        y (tensor): Objetivos esperados (batch_size, block_size).\n",
    "    \"\"\"\n",
    "    data_source = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_source) - block_size, (batch_size,))\n",
    "    \n",
    "    x = torch.stack([data_source[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data_source[i + 1 : i + block_size + 1] for i in ix])\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Evalúa la pérdida promedio en entrenamiento (train)\n",
    "    y validación (val) usando 'eval_iters' iteraciones.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rope_sin_cos(head_dim, block_size, base=10000):\n",
    "    \"\"\"\n",
    "    Construye tensores de seno y coseno para posiciones [0, block_size-1],\n",
    "    asumiendo que 'head_dim' es la dimensión de cada cabeza (debe ser par).\n",
    "\n",
    "    Args:\n",
    "        head_dim (int): Dimensión de cada cabeza (por ejemplo, 64).\n",
    "        block_size (int): Longitud máxima de la secuencia.\n",
    "        base (int): Base para la escala de frecuencia (usualmente 10000).\n",
    "\n",
    "    Returns:\n",
    "        sin (Tensor): Tensor de senos de forma (1, block_size, head_dim//2).\n",
    "        cos (Tensor): Tensor de cosenos de forma (1, block_size, head_dim//2).\n",
    "    \"\"\"\n",
    "    half_dim = head_dim // 2\n",
    "    # Calcular frecuencias para cada componente de la mitad\n",
    "    freq_seq = torch.arange(0, half_dim, dtype=torch.float32)\n",
    "    freq_seq = base ** (2 * freq_seq / (head_dim * 2))  # Escala según la dimensión\n",
    "\n",
    "    positions = torch.arange(block_size, dtype=torch.float32).unsqueeze(1)  # (block_size, 1)\n",
    "    div_term = positions / freq_seq.unsqueeze(0)  # (block_size, half_dim)\n",
    "    \n",
    "    sin = torch.sin(div_term).unsqueeze(0)  # (1, block_size, half_dim)\n",
    "    cos = torch.cos(div_term).unsqueeze(0)  # (1, block_size, half_dim)\n",
    "    return sin, cos\n",
    "\n",
    "def apply_rope(q, k, rope_sin, rope_cos):\n",
    "    \"\"\"\n",
    "    Aplica Rotary Positional Embeddings a los tensores de consulta y clave\n",
    "    de un único head.\n",
    "\n",
    "    Args:\n",
    "        q (Tensor): Tensor de consulta de forma (B, T, head_dim).\n",
    "        k (Tensor): Tensor de clave de forma (B, T, head_dim).\n",
    "        rope_sin (Tensor): Tensores de seno, de forma (1, block_size, head_dim//2).\n",
    "        rope_cos (Tensor): Tensores de coseno, de forma (1, block_size, head_dim//2).\n",
    "\n",
    "    Returns:\n",
    "        q_final, k_final (Tensors): Tensores rotados, de forma (B, T, head_dim).\n",
    "    \"\"\"\n",
    "    B, T, C = q.shape  # C = head_dim\n",
    "    half_dim = C // 2\n",
    "\n",
    "    # Dividir q y k en dos mitades\n",
    "    q_left, q_right = q[..., :half_dim], q[..., half_dim:]\n",
    "    k_left, k_right = k[..., :half_dim], k[..., half_dim:]\n",
    "    \n",
    "    # Seleccionar las posiciones (hasta T) y expandir para tener (B, T, half_dim)\n",
    "    sin_t = rope_sin[:, :T, :].expand(B, T, half_dim)\n",
    "    cos_t = rope_cos[:, :T, :].expand(B, T, half_dim)\n",
    "    \n",
    "    # Aplicar la rotación: \n",
    "    # q_rot = q_left * cos - q_right * sin\n",
    "    # q_pass = q_left * sin + q_right * cos\n",
    "    q_rot = q_left * cos_t - q_right * sin_t\n",
    "    q_pass = q_left * sin_t + q_right * cos_t\n",
    "    q_final = torch.cat((q_rot, q_pass), dim=-1)\n",
    "    \n",
    "    k_rot = k_left * cos_t - k_right * sin_t\n",
    "    k_pass = k_left * sin_t + k_right * cos_t\n",
    "    k_final = torch.cat((k_rot, k_pass), dim=-1)\n",
    "    \n",
    "    return q_final, k_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------\n",
    "# ENCABEZADO DEL MODELO\n",
    "# -------------------------------------------------------------------------------\n",
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Cabeza de self-attention con máscara causal y aplicación de RoPE.\n",
    "    \n",
    "    Esta implementación es para un único head; se espera que la dimensión\n",
    "    de entrada 'n_embd' en este módulo corresponda a 'head_dim' (por ejemplo, 64).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, head_size, block_size, dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_embd (int): Dimensión de entrada (debe ser igual a head_size para este head).\n",
    "            head_size (int): Tamaño de cada cabeza (por ejemplo, 64).\n",
    "            block_size (int): Longitud máxima de la secuencia.\n",
    "            dropout (float): Tasa de dropout.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        # Máscara causal: para que cada token no vea tokens futuros.\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, rope_sin, rope_cos):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Entrada de forma (B, T, head_dim), donde head_dim = head_size.\n",
    "            rope_sin, rope_cos (Tensors): Tensores de RoPE con forma (1, block_size, head_dim//2).\n",
    "        \n",
    "        Returns:\n",
    "            out (Tensor): Salida de la cabeza, de forma (B, T, head_size).\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # Aplicar RoPE a q y k\n",
    "        q_rot, k_rot = apply_rope(q, k, rope_sin, rope_cos)\n",
    "        \n",
    "        # Calcular atención escalada\n",
    "        scale = C ** -0.5\n",
    "        wei = q_rot @ k_rot.transpose(-2, -1) * scale  # (B, T, T)\n",
    "        \n",
    "        # Aplicar máscara causal\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Combinar la atención con los valores\n",
    "        out = wei @ v  # (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "# -------------------------------------------------\n",
    "# BLOQUES DEL TRANSFORMER (con FeedForward usando GELU)\n",
    "# -------------------------------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Ejemplo de MultiHeadAttention (sin details de RoPE, \n",
    "    pero podrías integrarlo aquí).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = n_embd // n_heads\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.wq = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.wk = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.wv = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Máscara causal\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Proyecciones\n",
    "        q_full = self.wq(x)  # (B, T, n_embd)\n",
    "        k_full = self.wk(x)\n",
    "        v_full = self.wv(x)\n",
    "\n",
    "        # Dividir heads\n",
    "        q = q_full.view(B, T, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        k = k_full.view(B, T, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "        v = v_full.view(B, T, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
    "\n",
    "        # Atención\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_dim**-0.5)\n",
    "        mask = self.tril[:T, :T]\n",
    "        wei = wei.masked_fill(mask==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        out = wei @ v  # (B, n_heads, T, head_dim)\n",
    "        # Reordenar\n",
    "        out = out.permute(0,2,1,3).contiguous()\n",
    "        out = out.view(B, T, self.n_heads*self.head_dim)\n",
    "        # Proyección final\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP tras la atención, usando GELU en lugar de ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.GELU(),           # Cambiamos ReLU -> GELU\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloque Transformer con:\n",
    "    - MultiHeadAttention\n",
    "    - FeedForward (con GELU)\n",
    "    - Residual connections\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.sa = MultiHeadAttention(n_embd, n_head, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Atención\n",
    "        x_ln = self.ln1(x)\n",
    "        attn_out = self.sa(x_ln)\n",
    "        x = x + attn_out\n",
    "        # FF\n",
    "        x_ln2 = self.ln2(x)\n",
    "        ff_out = self.ffwd(x_ln2)\n",
    "        x = x + ff_out\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de lenguaje con:\n",
    "    - Embeddings\n",
    "    - Bloques (Multi-Head + FeedForward con GELU)\n",
    "    - Funciones de generación con top-k / top-p / temperatura\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, block_size, n_embd, n_head, n_layer, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_embd = n_embd\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # Pila de bloques\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(n_embd, n_head, block_size, dropout) \n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        if T > self.block_size:\n",
    "            raise ValueError(\"Secuencia excede block_size\")\n",
    "\n",
    "        x = self.token_embedding_table(idx)  # (B, T, n_embd)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(\n",
    "        self, \n",
    "        idx, \n",
    "        max_new_tokens: int, \n",
    "        temperature: float=1.0, \n",
    "        top_k: int=None, \n",
    "        top_p: float=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Genera tokens con top-k / top-p sampling y temperatura.\n",
    "        idx: (B, T) secuencia de tokens\n",
    "        max_new_tokens: cuántos tokens generar\n",
    "        temperature: escalado de logits\n",
    "        top_k: mantiene sólo los k más probables\n",
    "        top_p: nucleus sampling, mantiene sólo tokens cumul>= top_p\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Cortar contexto\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            # Tomar logits del último token\n",
    "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "            # Ajustar por temperatura\n",
    "            if temperature != 1.0:\n",
    "                logits = logits / temperature\n",
    "\n",
    "            # Convertir a probabilidades\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, vocab_size)\n",
    "\n",
    "            # top-k\n",
    "            if (top_k is not None) and (top_k > 0):\n",
    "                # Coger los k mejores\n",
    "                values, indices = torch.topk(probs, top_k, dim=-1)\n",
    "                probs_topk = torch.zeros_like(probs).scatter_(-1, indices, values)\n",
    "                probs_topk = probs_topk / probs_topk.sum(dim=-1, keepdim=True)\n",
    "                probs = probs_topk\n",
    "\n",
    "            # top-p (nucleus sampling)\n",
    "            if (top_p is not None) and (0.0 < top_p < 1.0):\n",
    "                sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "                cumulative = torch.cumsum(sorted_probs, dim=-1)\n",
    "                # máscara booleana: qué tokens entran en el top_p\n",
    "                cutoff = (cumulative > top_p).float()\n",
    "                # shift para que el primero que supere top_p se incluya\n",
    "                cutoff = torch.roll(cutoff, 1, dims=-1)\n",
    "                cutoff[..., 0] = 0\n",
    "                # poner 0 en los tokens que exceden top_p\n",
    "                sorted_probs[cutoff.bool()] = 0.0\n",
    "                # renormalizar\n",
    "                sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "                # re-esparcir\n",
    "                probs_top_p = torch.zeros_like(probs).scatter_(-1, sorted_indices, sorted_probs)\n",
    "                probs = probs_top_p\n",
    "\n",
    "            # muestrear\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # concatenar\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# ENTRENAMIENTO AVANZADO\n",
    "# -------------------------------------------------\n",
    "\n",
    "def get_batch(train_data, val_data, batch_size, block_size, split='train', device='cuda'):\n",
    "    \"\"\"\n",
    "    Devuelve un batch (x, y) para el conjunto train o val.\n",
    "    \"\"\"\n",
    "    data_source = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_source) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_source[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data_source[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "def train_model():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # HIPERPARÁMETROS\n",
    "    batch_size = 64\n",
    "    block_size = 256\n",
    "    n_embd = 512 \n",
    "    n_head = 8\n",
    "    n_layer = 4\n",
    "    dropout = 0.2\n",
    "    max_iters = 7000\n",
    "    eval_interval = 200\n",
    "    lr = 3e-4\n",
    "    weight_decay = 1e-2\n",
    "    grad_accum_steps = 2  # Gradient Accumulation\n",
    "\n",
    "    # Instanciar el modelo\n",
    "    model = BigramLanguageModel(\n",
    "        vocab_size=vocab_size,\n",
    "        block_size=block_size,\n",
    "        n_embd=n_embd,\n",
    "        n_head=n_head,\n",
    "        n_layer=n_layer,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    # Define el número de pasos para el warmup\n",
    "    warmup_steps = 500  # Por ejemplo, los primeros 500 pasos\n",
    "\n",
    "    # max_iters es el total de iteraciones de entrenamiento (después del warmup)\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            # Warmup lineal: lr aumenta linealmente de 0 a 1\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        # Después del warmup, se aplica cosine decay\n",
    "        progress = float(current_step - warmup_steps) / float(max(1, max_iters - warmup_steps))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "    # Suponiendo que ya definiste el optimizador:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "    # Ejemplo de Cosine Scheduler + Warmup\n",
    "    # (PyTorch no trae un \"WarmupCosine\" nativo, \n",
    "    #  lo implementamos con un simple wrapper)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=max_iters, eta_min=1e-5)\n",
    "\n",
    "    def estimate_loss():\n",
    "        # evalúa la pérdida en train/val\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for split in ['train','val']:\n",
    "                losses = []\n",
    "                for _ in range(5):  # 5 mini-batches de eval\n",
    "                    xb, yb = get_batch(train_data, val_data, batch_size, block_size, split, device)\n",
    "                    _, loss_eval = model(xb, yb)\n",
    "                    losses.append(loss_eval.item())\n",
    "                out[split] = sum(losses)/len(losses)\n",
    "        model.train()\n",
    "        return out\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    \n",
    "    writer = SummaryWriter(log_dir=\"runs/experiment1\")\n",
    "    for iter in range(max_iters):\n",
    "        # Gradient Accumulation: \n",
    "        # Acumulamos grad en 'grad_accum_steps' sub-batches\n",
    "        loss_local = 0.0\n",
    "        for _ in range(grad_accum_steps):\n",
    "            xb, yb = get_batch(train_data, val_data, batch_size, block_size, 'train', device)\n",
    "            _, loss = model(xb, yb)\n",
    "            loss.backward()\n",
    "            loss_local += loss.item()\n",
    "        # Actualizamos (un step cada grad_accum_steps sub-batches)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss_local\n",
    "\n",
    "        # LR Scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Imprimir cada eval_interval\n",
    "        if iter % eval_interval == 0:\n",
    "            losses_eval = estimate_loss()\n",
    "            writer.add_scalar(\"Loss/train\", losses_eval[\"train\"], iter)\n",
    "            writer.add_scalar(\"Loss/val\", losses_eval[\"val\"], iter)\n",
    "            print(f\"Step {iter}: train_loss={losses_eval['train']:.4f}, val_loss={losses_eval['val']:.4f}\")\n",
    "\n",
    "\n",
    "        # (Opcional) Ajustar dropout dinámicamente (no estándar, \n",
    "        #  se hace manual, p.ej. reducirlo gradualmente):\n",
    "        # if iter < 500:\n",
    "        #     new_drop = 0.3\n",
    "        # else:\n",
    "        #     new_drop = 0.1\n",
    "        # for m in model.modules():\n",
    "        #     if isinstance(m, nn.Dropout):\n",
    "        #         m.p = new_drop\n",
    "\n",
    "    print(\"Entrenamiento finalizado\")\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'vocab_size': vocab_size,\n",
    "        'block_size': block_size,\n",
    "        'n_embd': n_embd,\n",
    "        'n_head': n_head,\n",
    "        'n_layer': n_layer\n",
    "    }\n",
    "    torch.save(checkpoint, 'bigram_model_checkpoint.pt')\n",
    "    print(\"Checkpoint guardado en 'bigram_model_checkpoint.pt'\")\n",
    "    return model\n",
    "\n",
    "trained_model = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Predicttext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
